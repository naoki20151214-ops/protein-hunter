import os
import json
import math
import time
import base64
import re
import traceback
from xml.sax.saxutils import escape
from dataclasses import dataclass
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import Dict, List, Tuple, Any, Optional

import requests
import gspread


# =========================
# Config (GitHub Secrets / Env)
# =========================
RAKUTEN_APP_ID = os.environ.get("RAKUTEN_APP_ID", "").strip()
RAKUTEN_AFFILIATE_ID = os.environ.get("RAKUTEN_AFFILIATE_ID", "").strip()

SHEET_ID = os.environ.get("SHEET_ID", "").strip()

# Recommended: store Base64 of the service account JSON in GitHub Secrets
GSPREAD_SERVICE_ACCOUNT_JSON_B64 = os.environ.get("GSPREAD_SERVICE_ACCOUNT_JSON_B64", "").strip()

DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL", "").strip()
HATENA_ID = os.environ.get("HATENA_ID", "").strip()
HATENA_API_KEY = os.environ.get("HATENA_API_KEY", "").strip()
HATENA_BLOG_ID = os.environ.get("HATENA_BLOG_ID", "").strip()

HATENA_API_BASE = "https://blog.hatena.ne.jp"

# Rakuten postageFlag (official): 0 = shipping included, 1 = shipping NOT included 
DEFAULT_SHIPPING_YEN = int(os.environ.get("DEFAULT_SHIPPING_YEN", "800"))

# Fetch more than we store, to avoid missing effective cheapest offers
FETCH_HITS = int(os.environ.get("FETCH_HITS", "100"))     # total offers fetched per canonical_id
STORE_HITS = int(os.environ.get("STORE_HITS", "20"))      # offers stored per canonical_id
RANKING_N = int(os.environ.get("RANKING_N", "20"))
HERO_K = int(os.environ.get("HERO_K", "3"))

REQUEST_SLEEP_SEC = float(os.environ.get("REQUEST_SLEEP_SEC", "1.0"))
STRICT_MODE = os.environ.get("STRICT_MODE", "false").strip().lower() in {"1", "true", "yes", "on"}

# Optional extra point boost (Phase2). Example: 0.02 for +2%
EXTRA_POINT_RATE = float(os.environ.get("EXTRA_POINT_RATE", "0.0"))  # 0.0..1.0

# Filtering
EXCLUDE_KEYWORDS = [k.strip() for k in os.environ.get(
    "EXCLUDE_KEYWORDS",
    # Stronger default list (safe-side). Extend anytime.
    "ã‚·ã‚§ã‚¤ã‚«ãƒ¼,ã‚·ã‚§ãƒ¼ã‚«ãƒ¼,ãƒœãƒˆãƒ«,ã‚¹ãƒ—ãƒ¼ãƒ³,è¨ˆé‡ã‚¹ãƒ—ãƒ¼ãƒ³,ãƒŸã‚­ã‚µãƒ¼,ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼,"
    "ãŠè©¦ã—,è©¦ä¾›å“,ã‚µãƒ³ãƒ—ãƒ«,ãƒˆãƒ©ã‚¤ã‚¢ãƒ«,å°åˆ†ã‘,å€‹åŒ…è£…,å°‘é‡,ãƒŸãƒ‹,"
    "è¨³ã‚ã‚Š,ä¸­å¤,ã‚¢ã‚¦ãƒˆãƒ¬ãƒƒãƒˆ,ç¦è¢‹,ã‚»ãƒƒãƒˆ,è©°ã‚åˆã‚ã›,ãƒãƒ©ã‚¨ãƒ†ã‚£,"
    "ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ãƒãƒ¼,ãƒãƒ¼,ã‚¯ãƒƒã‚­ãƒ¼,ãƒãƒ§ã‚³,ã‚·ãƒªã‚¢ãƒ«,ã‚°ãƒ©ãƒãƒ¼ãƒ©,"
    "ã‚²ã‚¤ãƒŠãƒ¼,å¢—é‡,ãƒã‚¹ã‚²ã‚¤ãƒŠãƒ¼,"
    "BCAA,EAA,ã‚¯ãƒ¬ã‚¢ãƒãƒ³,ã‚¢ãƒŸãƒé…¸,"
    "ã‚·ã‚§ã‚¤ã‚¯,ãƒ‰ãƒªãƒ³ã‚¯,é£²æ–™,ç¼¶,ç´™ãƒ‘ãƒƒã‚¯"
).split(",") if k.strip()]

# Capacity strict match is REQUIRED per your final spec
STRICT_CAPACITY_MATCH = True

# Rakuten endpoint (Ichiba Item Search)
RAKUTEN_ENDPOINT = "https://app.rakuten.co.jp/services/api/IchibaItem/Search/20220601"


# =========================
# Data models
# =========================
@dataclass
class MasterItem:
    canonical_id: str
    search_keyword: str
    brand: str
    capacity_kg: float
    protein_ratio: float  # 0.70 for 70% etc


@dataclass
class OfferRow:
    date: str
    canonical_id: str
    item_code: str
    shop_name: str
    raw_price: int
    shipping_cost: int
    point_rate: float
    protein_cost: float
    item_url: str
    item_name: str
    image_url: str


# =========================
# Helpers
# =========================
def jst_date() -> datetime.date:
    return datetime.now(ZoneInfo("Asia/Tokyo")).date()

def jst_today_str() -> str:
    return jst_date().isoformat()

def jst_now_iso() -> str:
    return datetime.now(ZoneInfo("Asia/Tokyo")).isoformat(timespec="seconds")


def choose_variant_jst(now: Optional[datetime] = None) -> Tuple[str, str, str, str, str, str]:
    dt = now.astimezone(ZoneInfo("Asia/Tokyo")) if now else datetime.now(ZoneInfo("Asia/Tokyo"))
    weekday = dt.weekday()  # Mon=0..Sun=6
    weekday_names = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]

    if weekday in {0, 2, 4}:  # Mon/Wed/Fri
        return (
            "A",
            "ä»Šæ—¥ãŒè²·ã„æ™‚",
            "30æ—¥æœ€å®‰æ°´æº–",
            "è£œå……ã™ã‚‹äººã¯ä»Šæ—¥ãŒå®‰å…¨ã€‚ãƒã‚¤ãƒ³ãƒˆæ¡ä»¶ã ã‘ç¢ºèªã—ã¦GOã€‚",
            dt.date().isoformat(),
            weekday_names[weekday],
        )
    return (
        "B",
        "é€ƒã™ã¨æã—ã‚„ã™ã„æ°´æº–",
        "æ€¥è½å¾Œã¯æˆ»ã‚Šã‚„ã™ã„",
        "ã“ã®æ°´æº–ã¯é•·ãç¶šã‹ãªã„ã“ã¨ãŒå¤šã„ã€‚å£²ã‚Šåˆ‡ã‚Œå‰ã«ç¢ºèªã€‚",
        dt.date().isoformat(),
        weekday_names[weekday],
    )


def normalize_image_url(url: str) -> str:
    image_url = (url or "").strip()
    if not image_url:
        return ""

    if image_url.startswith("//"):
        image_url = f"https:{image_url}"

    image_url = re.sub(r"^http://", "https://", image_url, flags=re.IGNORECASE)

    if re.search(r"([?&])_ex=\d+x\d+", image_url):
        image_url = re.sub(r"([?&])_ex=\d+x\d+", r"\1_ex=600x600", image_url)
    else:
        image_url = f"{image_url}&_ex=600x600" if "?" in image_url else f"{image_url}?_ex=600x600"

    return image_url


def pick_best_image_url(item: Dict[str, Any]) -> str:
    def first_image_url(raw: Any) -> str:
        if isinstance(raw, str):
            return normalize_image_url(raw)
        if isinstance(raw, dict):
            for key in ("imageUrl", "itemImageUrl", "url"):
                if raw.get(key):
                    return normalize_image_url(str(raw.get(key, "")))
            return ""
        if isinstance(raw, list):
            for elem in raw:
                selected = first_image_url(elem)
                if selected:
                    return selected
        return ""

    for key in (
        "mediumImageUrls",
        "smallImageUrls",
        "imageUrl",
        "itemImageUrl",
        "itemImageUrls",
    ):
        selected_url = first_image_url(item.get(key))
        if selected_url:
            return selected_url

    return ""


def shorten_item_name(name: str, limit: int = 40) -> str:
    text = (name or "").strip()
    if len(text) <= limit:
        return text
    return text[:limit] + "â€¦"

def safe_float(x: Any, default: float = 0.0) -> float:
    try:
        return float(x)
    except Exception:
        return default

def safe_int(x: Any, default: int = 0) -> int:
    try:
        return int(float(x))
    except Exception:
        return default

def discord_notify(title: str, lines: List[str]) -> None:
    if not DISCORD_WEBHOOK_URL:
        return
    content = f"**{title}**\n" + "\n".join(lines)
    content = content[:1800]
    try:
        resp = requests.post(DISCORD_WEBHOOK_URL, json={"content": content}, timeout=20)
        resp.raise_for_status()
    except Exception:
        print(f"ERROR discord: failed to send notification title={title[:80]}")
        traceback.print_exc()


@dataclass
class HatenaPostResult:
    ok: bool
    status_code: Optional[int]
    endpoint: str
    message: str


@dataclass
class PriceChangeReport:
    level: str
    today_price: int
    yesterday_price: Optional[int]
    diff_yen: Optional[int]
    diff_pct: Optional[float]
    is_30d_low: bool
    min_30d_price: Optional[int]
    variant: str
    variant_headline: str
    variant_reason: str
    variant_push_text: str
    date_jst: str
    weekday_jst: str
    image_url: str
    image_selected: bool
    short_item_name: str
    x_text: str
    hatena_markdown: str
    persona_summary_lines: List[str]


def build_hatena_service_endpoint() -> Optional[str]:
    if not HATENA_ID or not HATENA_BLOG_ID:
        return None
    return f"{HATENA_API_BASE}/{HATENA_ID}/{HATENA_BLOG_ID}/atom"


def build_hatena_entry_endpoint() -> Optional[str]:
    service_endpoint = build_hatena_service_endpoint()
    if not service_endpoint:
        return None
    return f"{service_endpoint}/entry"


def log_hatena_service_document(auth: Tuple[str, str], service_endpoint: str) -> None:
    try:
        resp = requests.get(service_endpoint, auth=auth, timeout=30)
        print(f"DEBUG hatena: service_document status={resp.status_code} endpoint={service_endpoint}")
        body_preview = (resp.text or "")[:500].replace("\n", " ").strip()
        if body_preview:
            print(f"DEBUG hatena: service_document body_preview={body_preview}")

        collection_hrefs = re.findall(r'<collection[^>]*href="([^"]+)"', resp.text or "")
        if collection_hrefs:
            print("DEBUG hatena: service_document collections=" + ", ".join(collection_hrefs))
        else:
            print("DEBUG hatena: service_document collections not found")
    except Exception:
        print("ERROR hatena: failed to fetch service document for diagnostics")
        traceback.print_exc()


def build_top3_markdown(best_offers: List[OfferRow]) -> str:
    lines = [
        f"## ğŸ† ä»Šæ—¥ã®ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ä¾¡æ ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚° â€“ {jst_today_str()}",
        "",
        f"- åŸºæº–: ã‚¿ãƒ³ãƒ‘ã‚¯è³ª1kgã‚ãŸã‚Šå®Ÿè³ªã‚³ã‚¹ãƒˆï¼ˆä¾¡æ ¼ + é€æ–™ - ãƒã‚¤ãƒ³ãƒˆï¼‰",
        "",
    ]

    if not best_offers:
        lines.extend([
            "### æœ¬æ—¥ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ",
            "- è©²å½“ãªã—ï¼ˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰",
        ])
        return "\n".join(lines)

    rank_icons = {1: "ğŸ¥‡", 2: "ğŸ¥ˆ", 3: "ğŸ¥‰"}
    for i, offer in enumerate(best_offers[:3], 1):
        rank_icon = rank_icons.get(i, "ğŸ…")
        lines.extend(
            [
                f"### {rank_icon} ç¬¬{i}ä½ï¼š**{offer.item_name}**",
                f"- å®Ÿè³ªã‚³ã‚¹ãƒˆï¼š{offer.protein_cost:,.0f}å†† / ã‚¿ãƒ³ãƒ‘ã‚¯è³ª1kg",
                f"- ä¾¡æ ¼è©³ç´°ï¼šæœ¬ä½“ {offer.raw_price:,}å†† / é€æ–™ {offer.shipping_cost:,}å†† / ãƒã‚¤ãƒ³ãƒˆ {offer.point_rate * 100:.1f}%",
                f"- ã‚·ãƒ§ãƒƒãƒ—ï¼š{offer.shop_name}",
                f"- ğŸ¯ ãƒªãƒ³ã‚¯ï¼šğŸ‘‰ [æ¥½å¤©ã§å•†å“ã‚’è¦‹ã‚‹]({offer.item_url})",
                "",
            ]
        )

    lines.extend(["---", "", "â€» ã“ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ã¯ã¦ãªãƒ–ãƒ­ã‚°AtomPubæŠ•ç¨¿ç”¨ã§ã™ã€‚"])

    return "\n".join(lines).strip()


PERSONA_SECTIONS: List[Tuple[str, str, bool]] = [
    ("â‘  åˆã‚ã¦ã®äºº", "ğŸŒ±", True),
    ("â‘¡ ã‚¬ãƒƒãƒ„ãƒªå¢—é‡ã—ãŸã„äºº", "ğŸ’ª", True),
    ("â‘¢ æœ¬æ°—ã§ç­‹è‚¥å¤§ã—ãŸã„äºº", "ğŸ‹ï¸", False),
    ("â‘£ ãƒ€ã‚¤ã‚¨ãƒƒãƒˆä¸­ã®äºº", "ğŸ¥—", True),
    ("â‘¤ 40ä»£ä»¥ä¸Šã®å¥åº·ç¶­æŒå±¤", "ğŸ§˜", False),
    ("â‘¥ å®¶è¨ˆé‡è¦–ãƒ»ã¾ã¨ã‚è²·ã„æ´¾", "ğŸ’´", False),
    ("â‘¦ å‘³é‡è¦–æ´¾", "ğŸ˜‹", False),
    ("â‘§ ç„¡æ·»åŠ å¿—å‘ãƒ»æˆåˆ†é‡è¦–æ´¾", "ğŸ§ª", False),
    ("â‘¨ é‹å‹•ã¯è»½ã‚ãƒ»å¥åº·ç›®çš„æ´¾", "ğŸš¶", False),
    ("â‘© ä»Šã ã‘å®‰ã„ç‹™ã„æ’ƒã¡æ´¾", "ğŸ¯", True),
]


def contains_any(text: str, patterns: List[str]) -> bool:
    src = (text or "").lower()
    return any(re.search(p, src, flags=re.IGNORECASE) for p in patterns)


def looks_official_or_major_shop(shop_name: str) -> bool:
    return contains_any(
        shop_name,
        [
            r"å…¬å¼",
            r"ã‚ªãƒ•ã‚£ã‚·ãƒ£ãƒ«",
            r"æœ¬åº—",
            r"æ¥½å¤©24",
            r"rakuten",
            r"amazon",
            r"yahoo",
            r"å¤§æ‰‹",
            r"ç›´å–¶",
        ],
    )


def choose_offer_from_candidates(
    candidates: List[OfferRow],
    fallback_sorted: List[OfferRow],
    used_urls: set,
    prefer_unused: bool = True,
) -> OfferRow:
    ordered = candidates + [o for o in fallback_sorted if o not in candidates]
    if prefer_unused:
        for offer in ordered:
            if offer.item_url not in used_urls:
                used_urls.add(offer.item_url)
                return offer
    chosen = ordered[0] if ordered else fallback_sorted[0]
    used_urls.add(chosen.item_url)
    return chosen


def assign_persona_sections(offers_for_this: List[OfferRow], prefer_unused: bool = True) -> Dict[str, OfferRow]:
    if not offers_for_this:
        return {}

    by_protein = sorted(offers_for_this, key=lambda x: x.protein_cost)
    by_price = sorted(offers_for_this, key=lambda x: x.raw_price)
    median_price_offer = by_price[len(by_price) // 2]
    median_protein_offer = by_protein[len(by_protein) // 2]
    used_urls: set = set()
    out: Dict[str, OfferRow] = {}

    def pick(section_name: str, candidates: List[OfferRow], fallback: List[OfferRow]) -> None:
        out[section_name] = choose_offer_from_candidates(candidates, fallback, used_urls, prefer_unused=prefer_unused)

    # â‘  åˆã‚ã¦
    pick("â‘  åˆã‚ã¦ã®äºº", [o for o in offers_for_this if contains_any(o.item_name, [r"1\s*kg", r"1\s*ã‚­ãƒ­", r"1ã‚­ãƒ­ã‚°ãƒ©ãƒ "])], by_price)

    # â‘¡ å¢—é‡
    pick("â‘¡ ã‚¬ãƒƒãƒ„ãƒªå¢—é‡ã—ãŸã„äºº", [o for o in offers_for_this if contains_any(o.item_name, [r"3\s*kg", r"3\s*ã‚­ãƒ­", r"3ã‚­ãƒ­ã‚°ãƒ©ãƒ "])], by_protein)

    # â‘¢ ç­‹è‚¥å¤§
    pick(
        "â‘¢ æœ¬æ°—ã§ç­‹è‚¥å¤§ã—ãŸã„äºº",
        [o for o in offers_for_this if contains_any(o.item_name, [r"wpi", r"ã‚¢ã‚¤ã‚½ãƒ¬ãƒ¼ãƒˆ", r"é«˜ãŸã‚“ã±ã", r"é«˜ã‚¿ãƒ³ãƒ‘ã‚¯"])],
        by_protein,
    )

    # â‘£ ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ
    pick(
        "â‘£ ãƒ€ã‚¤ã‚¨ãƒƒãƒˆä¸­ã®äºº",
        [o for o in offers_for_this if contains_any(o.item_name, [r"ä½è„‚è³ª", r"ä½ç³–è³ª", r"ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ", r"ç”˜ããªã„", r"ãƒ—ãƒ¬ãƒ¼ãƒ³"])],
        sorted(offers_for_this, key=lambda o: abs(o.raw_price - median_price_offer.raw_price)),
    )

    # â‘¤ 40ä»£
    protein_threshold = by_protein[max(0, len(by_protein) // 2 - 1)].protein_cost
    pick(
        "â‘¤ 40ä»£ä»¥ä¸Šã®å¥åº·ç¶­æŒå±¤",
        [o for o in offers_for_this if o.protein_cost <= protein_threshold and looks_official_or_major_shop(o.shop_name)],
        by_protein,
    )

    # â‘¥ å®¶è¨ˆ
    pick(
        "â‘¥ å®¶è¨ˆé‡è¦–ãƒ»ã¾ã¨ã‚è²·ã„æ´¾",
        [o for o in offers_for_this if contains_any(o.item_name, [r"3\s*kg", r"3\s*ã‚­ãƒ­", r"å¤§å®¹é‡", r"ã¾ã¨ã‚è²·ã„"])],
        sorted(offers_for_this, key=lambda o: (o.raw_price, o.protein_cost)),
    )

    # â‘¦ å‘³
    flavor_hit = [
        o for o in offers_for_this
        if contains_any(o.item_name, [r"ãƒãƒ§ã‚³", r"ãƒãƒ‹ãƒ©", r"ã‚¹ãƒˆãƒ­ãƒ™ãƒªãƒ¼", r"æŠ¹èŒ¶", r"é»’ç³–", r"ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆ", r"ãƒãƒ³ã‚´ãƒ¼", r"ãƒ”ãƒ¼ãƒ", r"ãƒ¡ãƒ­ãƒ³"])
    ]
    pick("â‘¦ å‘³é‡è¦–æ´¾", flavor_hit, sorted(offers_for_this, key=lambda o: (0 if o.image_url else 1, o.protein_cost)))

    # â‘§ ç„¡æ·»åŠ 
    pick(
        "â‘§ ç„¡æ·»åŠ å¿—å‘ãƒ»æˆåˆ†é‡è¦–æ´¾",
        [o for o in offers_for_this if contains_any(o.item_name, [r"ç„¡æ·»åŠ ", r"äººå·¥ç”˜å‘³æ–™ä¸ä½¿ç”¨", r"ç”˜å‘³æ–™ä¸ä½¿ç”¨", r"ä¿å­˜æ–™ä¸ä½¿ç”¨"])],
        sorted(offers_for_this, key=lambda o: (0 if looks_official_or_major_shop(o.shop_name) else 1, o.protein_cost)),
    )

    # â‘¨ è»½ã‚
    low_idx = int((len(by_price) - 1) * 0.1)
    high_idx = int((len(by_price) - 1) * 0.9)
    low_price = by_price[low_idx].raw_price
    high_price = by_price[high_idx].raw_price
    moderate = [o for o in offers_for_this if low_price <= o.raw_price <= high_price]
    pick("â‘¨ é‹å‹•ã¯è»½ã‚ãƒ»å¥åº·ç›®çš„æ´¾", moderate, sorted(offers_for_this, key=lambda o: abs(o.protein_cost - median_protein_offer.protein_cost)))

    # â‘© ç‹™ã„æ’ƒã¡
    max_point = max(o.point_rate for o in offers_for_this)
    pick("â‘© ä»Šã ã‘å®‰ã„ç‹™ã„æ’ƒã¡æ´¾", [o for o in offers_for_this if o.point_rate == max_point], sorted(offers_for_this, key=lambda o: (-o.point_rate, o.protein_cost)))

    return out


def build_persona_reason(section_name: str, offer: OfferRow, offers: List[OfferRow]) -> str:
    min_protein = min(o.protein_cost for o in offers)
    min_price = min(o.raw_price for o in offers)
    max_point = max(o.point_rate for o in offers)
    median_price = sorted(o.raw_price for o in offers)[len(offers) // 2]

    if section_name == "â‘  åˆã‚ã¦ã®äºº":
        if contains_any(offer.item_name, [r"1\s*kg", r"1\s*ã‚­ãƒ­", r"1ã‚­ãƒ­ã‚°ãƒ©ãƒ "]):
            return "1kgå‰å¾Œã®è¡¨è¨˜ã§é‡æ„ŸãŒã¤ã‹ã¿ã‚„ã™ãã€åˆå›ã§ã‚‚é¸ã³ã‚„ã™ã„ã€‚ä¾¡æ ¼å¸¯ã‚‚èª­ã¿ã‚„ã™ãã€å¤±æ•—ã—ã¥ã‚‰ã„ä¸€å“ã€‚"
        if offer.raw_price == min_price:
            return "ä»Šæ—¥ã®ä¾¡æ ¼å¸¯ã§æœ€å®‰ã‚¯ãƒ©ã‚¹ã€‚ã¾ãšå§‹ã‚ã‚‹1è¢‹ã¨ã—ã¦å‡ºè²»ã‚’æŠ‘ãˆã‚„ã™ã„ã€‚"
        return "ä¾¡æ ¼ã¨å˜ä¾¡ã®ãƒãƒ©ãƒ³ã‚¹ãŒå®‰å®šã—ã¦ã„ã¦ã€æœ€åˆã®1å“ã¨ã—ã¦ç„¡ç†ãªãç¶šã‘ã‚„ã™ã„ã€‚"
    if section_name == "â‘¡ ã‚¬ãƒƒãƒ„ãƒªå¢—é‡ã—ãŸã„äºº":
        if offer.protein_cost == min_protein:
            return "å˜ä¾¡ãŒä»Šæ—¥ã®æœ€å®‰ã€‚æ¯æ—¥ã—ã£ã‹ã‚Šé£²ã‚€å‰æã®äººã«åˆºã•ã‚‹æ§‹æˆã€‚"
        if offer.point_rate >= 0.05:
            return "ãƒã‚¤ãƒ³ãƒˆé‚„å…ƒãŒå¼·ãã€å®Ÿè³ªã‚³ã‚¹ãƒ‘ãŒã•ã‚‰ã«ä¼¸ã³ã‚„ã™ã„ã€‚å¢—é‡æœŸã®ç¶™ç¶šã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã‚„ã™ã„ã€‚"
        return "å®¹é‡å¯„ã‚Šã®å€™è£œã¨ã—ã¦æ—¥ã€…ã®æ¶ˆè²»ã«å‘ãã€‚é€æ–™è¾¼ã¿ã§ã‚‚ç·é¡ã§è¦‹ã¦å„ªä½ã‚’ä½œã‚Šã‚„ã™ã„ã€‚"
    if section_name == "â‘¢ æœ¬æ°—ã§ç­‹è‚¥å¤§ã—ãŸã„äºº":
        if contains_any(offer.item_name, [r"wpi", r"ã‚¢ã‚¤ã‚½ãƒ¬ãƒ¼ãƒˆ", r"é«˜ãŸã‚“ã±ã", r"é«˜ã‚¿ãƒ³ãƒ‘ã‚¯"]):
            return "é«˜ãŸã‚“ã±ãç³»ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å€™è£œã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é‡è¦–ã§æˆåˆ†è»¸ã‚’å„ªå…ˆã—ãŸã„æ—¥ã«åˆã†ã€‚"
        if offer.protein_cost <= min_protein * 1.05:
            return "å˜ä¾¡ãŒä¸Šä½æ°´æº–ãªã®ã§ã€æ‘‚å–é‡ã‚’å¢—ã‚„ã™å±€é¢ã§ã‚‚ç¶™ç¶šã—ã‚„ã™ã„ã€‚"
        return "ä¸Šä½ã‚³ã‚¹ãƒ‘å¸¯ã‹ã‚‰é¸å®šã€‚å®Ÿè¡Œã—ã‚„ã™ã•ã‚’é‡è¦–ã—ãŸç­‹è‚¥å¤§å‘ã‘ã®ç¾å®Ÿè§£ã€‚"
    if section_name == "â‘£ ãƒ€ã‚¤ã‚¨ãƒƒãƒˆä¸­ã®äºº":
        if contains_any(offer.item_name, [r"ä½è„‚è³ª", r"ä½ç³–è³ª", r"ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ", r"ç”˜ããªã„", r"ãƒ—ãƒ¬ãƒ¼ãƒ³"]):
            return "ãƒ€ã‚¤ã‚¨ãƒƒãƒˆå‘ã‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆã—ã¦é¸å®šã€‚å‘³ä»˜ã‘ãŒé‡ã™ããšã€èª¿æ•´ã—ã‚„ã™ã„ã€‚"
        if abs(offer.raw_price - median_price) <= max(300, median_price * 0.05):
            return "ä¾¡æ ¼ãŒä¸­ä½ä»˜è¿‘ã§æ¥µç«¯ã•ãŒå°‘ãªã„ã€‚ç¶šã‘ã‚‹å‰æã®ç½®ãæ›ãˆç”¨ã¨ã—ã¦æ‰±ã„ã‚„ã™ã„ã€‚"
        return "å˜ä¾¡ã¨ç·é¡ã®åã‚ŠãŒå°ã•ãã€æ¸›é‡ä¸­ã§ã‚‚ç®¡ç†ã—ã‚„ã™ã„ä¸€æœ¬ã€‚"
    if section_name == "â‘¤ 40ä»£ä»¥ä¸Šã®å¥åº·ç¶­æŒå±¤":
        if looks_official_or_major_shop(offer.shop_name):
            return "å…¬å¼ãƒ»å¤§æ‰‹å¯„ã‚Šã‚·ãƒ§ãƒƒãƒ—ã‚’å„ªå…ˆã€‚è³¼å…¥å‹•ç·šãŒã‚ã‹ã‚Šã‚„ã™ãã€ç¶™ç¶šã—ã‚„ã™ã„ã€‚"
        if offer.protein_cost <= min_protein * 1.1:
            return "å˜ä¾¡ãŒä¸Šä½50%ä»¥å†…ã®æ°´æº–ã§ã€ç„¡ç†ã®ãªã„ç¶™ç¶šã‚³ã‚¹ãƒˆã«å¯„ã›ã‚„ã™ã„ã€‚"
        return "ä¾¡æ ¼ãƒãƒ©ãƒ³ã‚¹é‡è¦–ã§é¸å®šã€‚ç¿’æ…£åŒ–ã‚’å´©ã—ã«ãã„å …å®Ÿãªå€™è£œã€‚"
    if section_name == "â‘¥ å®¶è¨ˆé‡è¦–ãƒ»ã¾ã¨ã‚è²·ã„æ´¾":
        if contains_any(offer.item_name, [r"3\s*kg", r"3\s*ã‚­ãƒ­", r"å¤§å®¹é‡", r"ã¾ã¨ã‚è²·ã„"]):
            return "å¤§å®¹é‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å„ªå…ˆã§ã€è²·ã„è¶³ã—å›æ•°ã‚’æ¸›ã‚‰ã—ã‚„ã™ã„ã€‚å®¶è¨ˆç®¡ç†ã®æ‰‹é–“ã‚‚æŠ‘ãˆã‚„ã™ã„ã€‚"
        if offer.raw_price == min_price:
            return "æœ¬ä½“ä¾¡æ ¼ãŒæœ€å®‰ã‚¯ãƒ©ã‚¹ã€‚ã¾ãšç·æ”¯å‡ºã‚’æŠ‘ãˆãŸã„æ—¥ã«ã¯æœ‰åŠ›ã€‚"
        return "ä¾¡æ ¼ã¨å˜ä¾¡ã®ä¸¡é¢ã‹ã‚‰å®¶è¨ˆå„ªå…ˆã§é¸å®šã€‚æ—¥æ¬¡é‹ç”¨ã§æ‰±ã„ã‚„ã™ã„ã€‚"
    if section_name == "â‘¦ å‘³é‡è¦–æ´¾":
        if contains_any(offer.item_name, [r"ãƒãƒ§ã‚³", r"ãƒãƒ‹ãƒ©", r"ã‚¹ãƒˆãƒ­ãƒ™ãƒªãƒ¼", r"æŠ¹èŒ¶", r"é»’ç³–", r"ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆ", r"ãƒãƒ³ã‚´ãƒ¼", r"ãƒ”ãƒ¼ãƒ", r"ãƒ¡ãƒ­ãƒ³"]):
            return "ãƒ•ãƒ¬ãƒ¼ãƒãƒ¼èªã‚’å«ã‚€å€™è£œã‚’å„ªå…ˆã€‚æ¯æ—¥é£²ã‚€å‰æã§ã‚‚é£½ãã«ãã•ã‚’ç‹™ãˆã‚‹ã€‚"
        if offer.image_url:
            return "ç”»åƒä»˜ãã§å‘³ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ´ã¿ã‚„ã™ã„å€™è£œã‚’å„ªå…ˆã€‚é¸ã¶ã‚¹ãƒˆãƒ¬ã‚¹ã‚’ä¸‹ã’ã‚„ã™ã„ã€‚"
        return "å‘³è»¸ã®å€™è£œãŒè–„ã„æ—¥ã¯ã€è¦‹ãŸç›®æƒ…å ±ã¨å˜ä¾¡ã®ãƒãƒ©ãƒ³ã‚¹ã§ç„¡é›£ã«é¸å®šã€‚"
    if section_name == "â‘§ ç„¡æ·»åŠ å¿—å‘ãƒ»æˆåˆ†é‡è¦–æ´¾":
        if contains_any(offer.item_name, [r"ç„¡æ·»åŠ ", r"äººå·¥ç”˜å‘³æ–™ä¸ä½¿ç”¨", r"ç”˜å‘³æ–™ä¸ä½¿ç”¨", r"ä¿å­˜æ–™ä¸ä½¿ç”¨"]):
            return "ç„¡æ·»åŠ ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆã€‚æˆåˆ†åŸºæº–ã§é¸ã³ãŸã„æ—¥ã«åˆ¤æ–­ã—ã‚„ã™ã„ã€‚"
        if looks_official_or_major_shop(offer.shop_name):
            return "å…¬å¼ãƒ»å¤§æ‰‹ã‚·ãƒ§ãƒƒãƒ—å¯„ã‚Šã‚’æ¡ç”¨ã€‚å•†å“æƒ…å ±ã®ç¢ºèªãŒã—ã‚„ã™ã„ç‚¹ã‚’é‡è¦–ã€‚"
        return "æˆåˆ†è¨´æ±‚ãŒå¼±ã„æ—¥ã¯ã‚·ãƒ§ãƒƒãƒ—ä¿¡é ¼åº¦ã¨ä¾¡æ ¼å®‰å®šæ€§ã‚’å„ªå…ˆã—ã¦é¸å®šã€‚"
    if section_name == "â‘¨ é‹å‹•ã¯è»½ã‚ãƒ»å¥åº·ç›®çš„æ´¾":
        if offer.raw_price == median_price:
            return "ä¾¡æ ¼ãŒä¸­å¤®å€¤ã§æ¥µç«¯ãªé«˜å®‰ã‚’é¿ã‘ã‚‰ã‚Œã‚‹ã€‚è»½ã‚é‹å‹•ã®è£œåŠ©ã¨ã—ã¦ç¶šã‘ã‚„ã™ã„ã€‚"
        if offer.protein_cost <= min_protein * 1.15:
            return "å˜ä¾¡ãŒä¸­åº¸ã€œä¸Šä½å¸¯ã§ã€éä¸è¶³ã®ãªã„ã‚³ã‚¹ãƒ‘ã‚’å–ã‚Šã‚„ã™ã„ã€‚"
        return "é«˜ã™ããšå®‰ã™ããªã„å¸¯ã‹ã‚‰é¸å®šã€‚å¥åº·ç¶­æŒç›®çš„ã§ã‚‚ä½¿ã„ã‚„ã™ã„ã€‚"
    if section_name == "â‘© ä»Šã ã‘å®‰ã„ç‹™ã„æ’ƒã¡æ´¾":
        if offer.point_rate == max_point:
            return "æœ¬æ—¥ã®ãƒã‚¤ãƒ³ãƒˆé‚„å…ƒãŒæœ€å¤§ã‚¯ãƒ©ã‚¹ã€‚å®Ÿè³ªè² æ‹…ã‚’ç‹™ã£ã¦å–ã‚Šã«ã„ã‘ã‚‹æ§‹æˆã€‚"
        if offer.protein_cost == min_protein:
            return "é‚„å…ƒã‚’é™¤ã„ã¦ã‚‚å˜ä¾¡ãŒæœ€å®‰æ°´æº–ã€‚ã‚¿ã‚¤ãƒŸãƒ³ã‚°è²·ã„ã®ä¸»è»¸ã«ã—ã‚„ã™ã„ã€‚"
        return "é‚„å…ƒã¨å˜ä¾¡ã®åˆç®—ã§ãŠå¾—æ„Ÿã‚’å„ªå…ˆã€‚çŸ­æœŸã®ç‹™ã„æ’ƒã¡ã«å‘ãå€™è£œã€‚"
    return "å½“æ—¥ã®ä¾¡æ ¼ãƒ»é‚„å…ƒæ¡ä»¶ã‹ã‚‰æ©Ÿæ¢°é¸å®šã—ãŸãŠã™ã™ã‚ã§ã™ã€‚"


def build_persona_sections_markdown(assignments: Dict[str, OfferRow], offers: List[OfferRow]) -> Tuple[List[str], List[str]]:
    markdown_lines: List[str] = ["## äººåˆ¥ãŠã™ã™ã‚ï¼ˆä»Šæ—¥ã®10æ ï¼‰", ""]
    discord_lines: List[str] = []
    image_sections = {"â‘  åˆã‚ã¦ã®äºº", "â‘¡ ã‚¬ãƒƒãƒ„ãƒªå¢—é‡ã—ãŸã„äºº", "â‘£ ãƒ€ã‚¤ã‚¨ãƒƒãƒˆä¸­ã®äºº", "â‘© ä»Šã ã‘å®‰ã„ç‹™ã„æ’ƒã¡æ´¾"}

    for section_name, emoji, show_image in PERSONA_SECTIONS:
        offer = assignments.get(section_name)
        if not offer:
            continue
        reason = build_persona_reason(section_name, offer, offers)
        point_pct = offer.point_rate * 100.0
        markdown_lines.extend(
            [
                f"## {emoji} {section_name}",
                f"- ãŠã™ã™ã‚: **{shorten_item_name(offer.item_name, 52)}**",
                f"- ç†ç”±: {reason}",
                f"- å®Ÿè³ª: **{offer.protein_cost:,.0f}å††/kg**ï½œä¾¡æ ¼: {offer.raw_price:,}å††ï½œpt: {point_pct:.1f}%ï½œã‚·ãƒ§ãƒƒãƒ—: {offer.shop_name}",
                f"**ğŸ‘‰ [å•†å“ã‚’è¦‹ã«è¡Œã]({offer.item_url})**",
            ]
        )
        if show_image and section_name in image_sections and offer.image_url:
            markdown_lines.append(f"![å•†å“ç”»åƒ]({offer.image_url})")
        markdown_lines.append("")
        discord_lines.append(f"- {section_name}: {shorten_item_name(offer.item_name, 26)}ï½œ{offer.protein_cost:,.0f}å††/kgï½œ{offer.item_url}")

    return markdown_lines, discord_lines


def is_explosion_3kg_target(master: MasterItem) -> bool:
    cid = (master.canonical_id or "").lower()
    kw = (master.search_keyword or "").lower()
    brand = (master.brand or "").lower()
    name_hit = "explosion" in cid or "explosion" in kw or "ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³" in master.search_keyword or "ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³" in master.brand
    return name_hit and abs(master.capacity_kg - 3.0) < 1e-9


def read_price_history_daily_min(hist_ws, canonical_id: str) -> Dict[str, int]:
    rows = hist_ws.get_all_records()
    out: Dict[str, int] = {}
    for r in rows:
        cid = str(r.get("canonical_id", "")).strip()
        if cid != canonical_id:
            continue
        day = str(r.get("date", "")).strip()
        if not day:
            continue
        raw_price = safe_int(r.get("raw_price", math.inf), math.inf)
        if raw_price == math.inf:
            continue
        prev = out.get(day)
        if prev is None or raw_price < prev:
            out[day] = raw_price
    return out


def choose_level(diff_yen: Optional[int], diff_pct: Optional[float], is_30d_low: bool) -> str:
    if is_30d_low:
        return "big_drop"
    if diff_yen is None or diff_pct is None:
        return "normal"
    if diff_pct <= -5.0 or diff_yen <= -500:
        return "big_drop"
    if diff_pct <= -3.0 or diff_yen <= -300:
        return "drop"
    return "normal"


def build_marketing_report(
    master: MasterItem,
    best_offer: OfferRow,
    hist_ws,
    today: str,
    yesterday: str,
    ranking_offers: Optional[List[OfferRow]] = None,
) -> PriceChangeReport:
    daily_min = read_price_history_daily_min(hist_ws, master.canonical_id)
    today_price = best_offer.raw_price
    yesterday_price = daily_min.get(yesterday)

    diff_yen: Optional[int] = None
    diff_pct: Optional[float] = None
    if yesterday_price and yesterday_price > 0:
        diff_yen = today_price - yesterday_price
        diff_pct = (diff_yen / yesterday_price) * 100.0

    start_date = (jst_date() - timedelta(days=29)).isoformat()
    recent_prices = [p for d, p in daily_min.items() if start_date <= d <= today]
    if recent_prices:
        min_30d_price = min(recent_prices)
        is_30d_low = today_price <= min_30d_price
    else:
        min_30d_price = None
        is_30d_low = False

    level = choose_level(diff_yen, diff_pct, is_30d_low)
    variant, variant_headline, variant_reason, variant_push_text, date_jst, weekday_jst = choose_variant_jst()
    short_name = shorten_item_name(best_offer.item_name)

    diff_label = (
        f"å‰æ—¥æ¯” {diff_yen:+,}å†† ({diff_pct:+.1f}%)"
        if diff_yen is not None and diff_pct is not None
        else "å‰æ—¥æ¯” ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
    )
    low30_label = f"30æ—¥æœ€å®‰ {min_30d_price:,}å††" if min_30d_price is not None else "30æ—¥æœ€å®‰ ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
    diff_inline = (
        f"{diff_yen:+,}å††ï¼ˆ{diff_pct:+.1f}%ï¼‰"
        if diff_yen is not None and diff_pct is not None
        else "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
    )
    low30_flag = "æ›´æ–°" if is_30d_low else "æœªæ›´æ–°"

    x_text = "\n".join(
        [
            "ã€Rakuten Protein Trackerã€‘",
            "ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³ 3kg ä¾¡æ ¼ãƒã‚§ãƒƒã‚¯",
            f"ä»Šæ—¥ã®æœ€å®‰: {today_price:,}å††",
            diff_label,
            f"å¤‰å‹•ãƒ¬ãƒ™ãƒ«: {level}",
            f"{low30_label} / {low30_flag}",
            variant_push_text,
            best_offer.item_url,
            "#æ¥½å¤©å¸‚å ´ #ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ #ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³",
        ]
    )

    image_block_lines: List[str] = []
    if best_offer.image_url:
        image_block_lines = [f"![å•†å“ç”»åƒ]({best_offer.image_url})", ""]

    ranking_sections: List[str] = []
    persona_section_lines: List[str] = []
    persona_summary_lines: List[str] = []
    if ranking_offers is not None:
        persona_assignments = assign_persona_sections(ranking_offers, prefer_unused=True)
        assignment_summary = {
            section: (offer.item_url or offer.canonical_id)
            for section, offer in persona_assignments.items()
        }
        print("INFO section assignment summary:", json.dumps(assignment_summary, ensure_ascii=False))
        persona_section_lines, persona_summary_lines = build_persona_sections_markdown(persona_assignments, ranking_offers)

        hero_offers = ranking_offers[:HERO_K]
        top_offers = ranking_offers[:RANKING_N]

        if hero_offers:
            medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
            ranking_sections.extend(["## ä»Šæ—¥ã®æ¨ã—ï¼ˆTOP3ï¼‰", ""])
            for i, offer in enumerate(hero_offers):
                medal = medals[i] if i < len(medals) else "ğŸ…"
                point_pct = (offer.point_rate if offer.point_rate is not None else 0.0) * 100.0
                ranking_sections.append(f"### {medal} {shorten_item_name(offer.item_name, 60)}")
                if offer.item_url:
                    ranking_sections.append(f"**ğŸ‘‰ [å•†å“ã‚’è¦‹ã«è¡Œã]({offer.item_url})**")
                if offer.image_url:
                    ranking_sections.append(f"![å•†å“ç”»åƒ]({offer.image_url})")
                ranking_sections.extend(
                    [
                        f"- å®Ÿè³ªå˜ä¾¡: **{offer.protein_cost:,.0f}å††/kg**",
                        f"- ä¾¡æ ¼: {offer.raw_price:,}å††ï¼ˆé€æ–™ {offer.shipping_cost:,}å††ï¼‰",
                        f"- pt: {point_pct:.1f}%",
                        f"- ã‚·ãƒ§ãƒƒãƒ—: {offer.shop_name or ''}",
                    ]
                )
                if offer.item_url:
                    ranking_sections.append(f"**ğŸ‘‰ [æ¥½å¤©ã§ä¾¡æ ¼ã¨åœ¨åº«ã‚’ç¢ºèªã™ã‚‹]({offer.item_url})**")
                ranking_sections.append("")

        if top_offers:
            ranking_sections.extend(["## ä»Šæ—¥ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆTOP20ï¼‰", ""])
            for rank, offer in enumerate(top_offers, 1):
                ranking_sections.append(
                    f"- {rank}. {shorten_item_name(offer.item_name, 60)}ï½œ**{offer.protein_cost:,.0f}å††/kg**ï½œ{offer.shop_name or ''}"
                )
                if offer.item_url:
                    ranking_sections.append(f"  - **ğŸ‘‰ [å•†å“ã‚’è¦‹ã«è¡Œã]({offer.item_url})**")
            ranking_sections.append("")

    hatena_markdown = "\n".join(
        image_block_lines + [
            f"ğŸ”¥ åˆ¤å®šï¼š{variant_headline}ï¼ˆ{variant_reason}ï¼‰",
            f"å®Ÿè³ªï¼š{today_price:,}å††/kgï½œå‰æ—¥æ¯”ï¼š{diff_inline}ï½œ30æ—¥æœ€å®‰ï¼š{low30_flag}",
            "ğŸ‘‰ ä¾¡æ ¼ã¨åœ¨åº«ã¯ä¸‹ã®ãƒœã‚¿ãƒ³ã‹ã‚‰ç¢ºèª",
            "",
            f"# ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³3kg ä¾¡æ ¼é€Ÿå ±ï¼ˆ{today}ï¼‰",
            "",
            f"**{variant_headline}**",
            "",
            f"- ä»Šæ—¥æœ€å®‰: **{today_price:,}å††/kg**",
            f"- å‰æ—¥æ¯”: **{diff_inline}**",
            f"- 30æ—¥æœ€å®‰: **{low30_flag}**ï¼ˆ{f'{min_30d_price:,}å††' if min_30d_price is not None else 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³'}ï¼‰",
            "",
            "## ä»Šæ—¥ã®çµè«–",
            f"- åˆ¤å®š: **{variant_headline}**",
            f"- ç†ç”±: {variant_reason}",
            "",
        ] + persona_section_lines + ranking_sections + [
            "## ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿",
            f"- å•†å“å: {short_name}",
            f"- ã‚·ãƒ§ãƒƒãƒ—: {best_offer.shop_name}",
            f"- ä»Šæ—¥ã®å®Ÿè³ªä¾¡æ ¼: **{today_price:,}å††/kg**",
            f"- å‰æ—¥æ¯”: **{diff_inline}**",
            f"- 30æ—¥æœ€å®‰: **{low30_flag}**ï¼ˆ{f'{min_30d_price:,}å††' if min_30d_price is not None else 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³'}ï¼‰",
            "",
            "## è²·ã„æ™‚ã‚³ãƒ¡ãƒ³ãƒˆ",
            variant_push_text,
            "",
            "## CTA",
            "### âœ… ä»Šã™ãç¢ºèª",
            f"**ğŸ‘‰ [æ¥½å¤©ã§ä¾¡æ ¼ã¨åœ¨åº«ã‚’ç¢ºèªã™ã‚‹]({best_offer.item_url})**",
            "",
            "## æ³¨æ„æ›¸ã",
            "â€» ä¾¡æ ¼ãƒ»ãƒã‚¤ãƒ³ãƒˆãƒ»åœ¨åº«ã¯å¤‰å‹•ã—ã¾ã™ã€‚è³¼å…¥å‰ã«æ¥½å¤©ã®å•†å“ãƒšãƒ¼ã‚¸ã§æœ€æ–°æƒ…å ±ã‚’ã”ç¢ºèªãã ã•ã„ã€‚",
        ]
    )

    return PriceChangeReport(
        level=level,
        today_price=today_price,
        yesterday_price=yesterday_price,
        diff_yen=diff_yen,
        diff_pct=diff_pct,
        is_30d_low=is_30d_low,
        min_30d_price=min_30d_price,
        variant=variant,
        variant_headline=variant_headline,
        variant_reason=variant_reason,
        variant_push_text=variant_push_text,
        date_jst=date_jst,
        weekday_jst=weekday_jst,
        image_url=best_offer.image_url,
        image_selected=bool(best_offer.image_url),
        short_item_name=short_name,
        x_text=x_text,
        hatena_markdown=hatena_markdown,
        persona_summary_lines=persona_summary_lines,
    )


def post_top3_to_hatena(markdown_body: str) -> HatenaPostResult:
    if not HATENA_ID or not HATENA_API_KEY or not HATENA_BLOG_ID:
        msg = "skipped post because HATENA_ID/HATENA_API_KEY/HATENA_BLOG_ID is missing"
        print(f"WARNING hatena: {msg}")
        return HatenaPostResult(ok=False, status_code=None, endpoint="", message=msg)

    entry_endpoint = build_hatena_entry_endpoint()
    service_endpoint = build_hatena_service_endpoint()
    if not entry_endpoint or not service_endpoint:
        msg = "skipped post because endpoint could not be built"
        print(f"WARNING hatena: {msg}")
        return HatenaPostResult(ok=False, status_code=None, endpoint="", message=msg)

    title = f"ã€ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³ä¾¡æ ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘{jst_today_str()}"
    atom_xml = f"""<?xml version=\"1.0\" encoding=\"utf-8\"?>
<entry xmlns=\"http://www.w3.org/2005/Atom\" xmlns:app=\"http://www.w3.org/2007/app\" xmlns:hatena=\"http://www.hatena.ne.jp/info/xmlns#\">
  <title>{escape(title)}</title>
  <author><name>{escape(HATENA_ID)}</name></author>
  <hatena:syntax>markdown</hatena:syntax>
  <content type=\"text/plain\">{escape(markdown_body)}</content>
  <app:control>
    <app:draft>yes</app:draft>
  </app:control>
</entry>
"""

    try:
        print(f"INFO hatena: posting draft endpoint={entry_endpoint}")
        resp = requests.post(
            entry_endpoint,
            data=atom_xml.encode("utf-8"),
            auth=(HATENA_ID, HATENA_API_KEY),
            headers={"Content-Type": "application/xml; charset=utf-8"},
            timeout=30,
        )
        print(f"INFO hatena: draft post response status={resp.status_code} endpoint={entry_endpoint}")
        if resp.status_code >= 400:
            body_preview = (resp.text or "")[:500].replace("\n", " ").strip()
            msg = f"draft post failed body={body_preview}"
            print(f"ERROR hatena: {msg} status={resp.status_code} endpoint={entry_endpoint}")
            if resp.status_code == 404:
                log_hatena_service_document((HATENA_ID, HATENA_API_KEY), service_endpoint)
            return HatenaPostResult(ok=False, status_code=resp.status_code, endpoint=entry_endpoint, message=msg)
        print(f"INFO hatena: draft post succeeded status={resp.status_code} endpoint={entry_endpoint}")
        return HatenaPostResult(ok=True, status_code=resp.status_code, endpoint=entry_endpoint, message="draft post succeeded")
    except Exception as e:
        msg = f"failed to post top3 draft: {e}"
        print(f"ERROR hatena: {msg} endpoint={entry_endpoint}")
        traceback.print_exc()
        return HatenaPostResult(ok=False, status_code=None, endpoint=entry_endpoint, message=msg)


# =========================
# Google Sheets
# =========================
def load_service_account_dict_b64() -> dict:
    if not (SHEET_ID and GSPREAD_SERVICE_ACCOUNT_JSON_B64):
        raise RuntimeError("Missing SHEET_ID or GSPREAD_SERVICE_ACCOUNT_JSON_B64")
    raw = base64.b64decode(GSPREAD_SERVICE_ACCOUNT_JSON_B64.encode("utf-8")).decode("utf-8")
    return json.loads(raw)

def open_sheets():
    masked_sheet_id = f"{SHEET_ID[:4]}...{SHEET_ID[-4:]}" if len(SHEET_ID) >= 8 else "(masked)"
    print(f"DEBUG sheet: opening sheet... sheet_id={masked_sheet_id}")
    creds_dict = load_service_account_dict_b64()
    gc = gspread.service_account_from_dict(creds_dict)
    print("DEBUG sheet: gspread authentication success")
    sh = gc.open_by_key(SHEET_ID)

    master_ws = sh.worksheet("Master_List")
    print(f"DEBUG sheet: worksheet name={master_ws.title}")
    hist_ws = sh.worksheet("Price_History")
    print(f"DEBUG sheet: worksheet name={hist_ws.title}")

    # Min_Summary worksheet (create if missing)
    try:
        min_ws = sh.worksheet("Min_Summary")
        print(f"DEBUG sheet: worksheet name={min_ws.title}")
    except gspread.exceptions.WorksheetNotFound:
        print("DEBUG sheet: worksheet name=Min_Summary (not found, creating)")
        min_ws = sh.add_worksheet(title="Min_Summary", rows=2000, cols=10)
        min_ws.append_row(
            ["date", "canonical_id", "min_cost", "min_shop", "min_url", "updated_at"],
            value_input_option="RAW",
        )
        print(f"DEBUG sheet: worksheet name={min_ws.title} (created)")

    return master_ws, hist_ws, min_ws

def read_master(master_ws) -> List[MasterItem]:
    rows = master_ws.get_all_records()
    items: List[MasterItem] = []
    for r in rows:
        cid = str(r.get("canonical_id", "")).strip()
        kw = str(r.get("search_keyword", "")).strip()
        if not cid or not kw:
            continue
        items.append(
            MasterItem(
                canonical_id=cid,
                search_keyword=kw,
                brand=str(r.get("brand", "")).strip(),
                capacity_kg=safe_float(r.get("capacity_kg", 0)),
                protein_ratio=safe_float(r.get("protein_ratio", 0)),
            )
        )
    return items

def ensure_history_headers(hist_ws) -> None:
    existing = hist_ws.get_all_values()
    if existing:
        return
    hist_ws.append_row(
        [
            "date",
            "canonical_id",
            "item_code",
            "shop_name",
            "raw_price",
            "shipping_cost",
            "point_rate",
            "protein_cost",
            "item_url",
            "item_name",
        ],
        value_input_option="RAW",
    )

def append_history(hist_ws, offer_rows: List[OfferRow]) -> None:
    if not offer_rows:
        return
    ensure_history_headers(hist_ws)
    values = [
        [
            o.date,
            o.canonical_id,
            o.item_code,
            o.shop_name,
            o.raw_price,
            o.shipping_cost,
            round(o.point_rate, 6),
            round(o.protein_cost, 6),
            o.item_url,
            o.item_name,
        ]
        for o in offer_rows
    ]
    print(f"DEBUG sheet: appending {len(values)} rows")
    try:
        hist_ws.append_rows(values, value_input_option="RAW")
    except Exception:
        print("ERROR sheet: append_rows failed")
        traceback.print_exc()
        raise
    print("DEBUG sheet: append success")

def read_min_summary(min_ws, target_date: str) -> Dict[str, Tuple[float, str, str]]:
    rows = min_ws.get_all_records()
    out: Dict[str, Tuple[float, str, str]] = {}
    for r in rows:
        if str(r.get("date", "")).strip() != target_date:
            continue
        cid = str(r.get("canonical_id", "")).strip()
        if not cid:
            continue
        out[cid] = (
            safe_float(r.get("min_cost", math.inf), math.inf),
            str(r.get("min_shop", "")).strip(),
            str(r.get("min_url", "")).strip(),
        )
    return out

def read_alltime_min(min_ws) -> Dict[str, Tuple[float, str, str]]:
    rows = min_ws.get_all_records()
    out: Dict[str, Tuple[float, str, str]] = {}
    for r in rows:
        cid = str(r.get("canonical_id", "")).strip()
        if not cid:
            continue
        cost = safe_float(r.get("min_cost", math.inf), math.inf)
        shop = str(r.get("min_shop", "")).strip()
        url = str(r.get("min_url", "")).strip()
        prev = out.get(cid)
        if prev is None or cost < prev[0]:
            out[cid] = (cost, shop, url)
    return out

def upsert_today_min(min_ws, date: str, cid: str, min_cost: float, min_shop: str, min_url: str) -> None:
    """
    Upsert by (date, canonical_id).
    Uses a simple scan; Min_Summary is small (20 items/day), so stays fast.
    """
    values = min_ws.get_all_values()
    target_row = None

    # header row = 1
    for row_idx in range(2, len(values) + 1):
        row = values[row_idx - 1]
        if len(row) >= 2 and row[0] == date and row[1] == cid:
            target_row = row_idx
            break

    updated_at = jst_now_iso()
    if target_row:
        # gspread warningå¯¾ç­–: updateã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°(values/range_name)ã§çµ±ä¸€ã™ã‚‹
        min_ws.update(
            values=[[str(min_cost), min_shop, min_url, updated_at]],
            range_name=f"C{target_row}:F{target_row}",
        )
    else:
        min_ws.append_row([date, cid, str(min_cost), min_shop, min_url, updated_at], value_input_option="RAW")


# =========================
# Rakuten API
# =========================
def rakuten_search_page(keyword: str, page: int, hits: int) -> Tuple[List[Dict[str, Any]], int]:
    if not RAKUTEN_APP_ID:
        raise RuntimeError("Missing RAKUTEN_APP_ID")

    params = {
        "applicationId": RAKUTEN_APP_ID,
        "keyword": keyword,
        "hits": max(1, min(30, hits)),
        "page": page,
        "sort": "+itemPrice",
        "format": "json",
        "formatVersion": 2,
    }
    if RAKUTEN_AFFILIATE_ID:
        params["affiliateId"] = RAKUTEN_AFFILIATE_ID

    resp = requests.get(RAKUTEN_ENDPOINT, params=params, timeout=30)
    resp.raise_for_status()

    data = resp.json()

    print("DEBUG http:", resp.status_code, "keys:", list(data.keys())[:10])

    total_count = safe_int(data.get("count", 0), 0) if isinstance(data, dict) else 0

    # formatVersion=2 style
    if isinstance(data, dict) and data.get("items"):
        return data["items"], total_count

    # old style variants
    if isinstance(data, dict) and data.get("Items"):
        items = data["Items"]
        if not items:
            return []
        first = items[0]
        # {"Items":[{"Item":{...}}, ...]}
        if isinstance(first, dict) and "Item" in first:
            return [x["Item"] for x in items if isinstance(x, dict) and "Item" in x], total_count
        # {"Items":[{...}, ...]}  â† ã“ã£ã¡ã‚‚ã‚ã‚‹
        if isinstance(first, dict):
            return items, total_count

    # API error payload
    if isinstance(data, dict) and (data.get("error") or data.get("error_description")):
        raise RuntimeError(f"Rakuten API error: {data.get('error')} {data.get('error_description')}")

    return [], total_count
    
def rakuten_search_multi_pages(keyword: str, total_hits: int) -> Tuple[List[Dict[str, Any]], int]:
    all_items: List[Dict[str, Any]] = []
    remaining = total_hits
    page = 1
    api_total_count = 0

    while remaining > 0:
        hits = min(30, remaining)
        items, total_count = rakuten_search_page(keyword, page=page, hits=hits)
        if page == 1:
            api_total_count = total_count
        if not items:
            break

        all_items.extend(items)
        remaining -= len(items)

        if len(items) < hits:
            break

        page += 1
        if page > 10:
            break

        time.sleep(0.3)

    return all_items, api_total_count

# =========================
# Filtering / Compute
# =========================
def looks_like_garbage(item_name: str) -> bool:
    name = item_name or ""
    return any(k in name for k in EXCLUDE_KEYWORDS)
    
def _norm_name(s: str) -> str:
    s = (s or "").lower()
    # å…¨è§’æ•°å­—â†’åŠè§’
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789"))
    # å…¨è§’è‹±å­—ã£ã½ã„ã®ã‚’åŠè§’ã¸å¯„ã›ã‚‹ï¼ˆæœ€ä½é™ï¼‰
    s = s.replace("ï½‹", "k").replace("ï½‡", "g").replace("ï¼«", "k").replace("ï¼§", "g")
    # ã‚¹ãƒšãƒ¼ã‚¹é¡ã‚’æ¶ˆã™
    s = re.sub(r"\s+", "", s)
    return s

def capacity_strict_match(master: MasterItem, item_name: str) -> bool:
    if not STRICT_CAPACITY_MATCH:
        return False
    if master.capacity_kg <= 0:
        return True

    name = _norm_name(item_name)
    kg = master.capacity_kg

    if kg >= 1.0:
        n = int(round(kg))
        # ä¾‹: 3kg / 3kgÃ—1 / 3kgx1 / 3kg(ã€œ) / 3kgå…¥ã‚Š ãªã©ã‚’è¨±å®¹
        return re.search(rf"{n}kg($|[Ã—x\(\)0-9]|å…¥ã‚Š|ï¾Šï¾Ÿï½¯ï½¸|è¢‹|å€‹)", name) is not None or f"{n}kg" in name

    grams = int(round(kg * 1000))
    return re.search(rf"{grams}g($|[Ã—x\(\)0-9]|å…¥ã‚Š|ï¾Šï¾Ÿï½¯ï½¸|è¢‹|å€‹)", name) is not None or f"{grams}g" in name
    
def compute_offer(master: MasterItem, item: Dict[str, Any]) -> Optional[OfferRow]:
    date = jst_today_str()
    item_code = str(item.get("itemCode", "")).strip()
    shop_name = str(item.get("shopName", "")).strip()
    item_url = str(item.get("itemUrl", "")).strip()
    item_name = str(item.get("itemName", "")).strip()
    image_url = pick_best_image_url(item)

    raw_price = safe_int(item.get("itemPrice", 0), 0)
    if not item_code or not shop_name or raw_price <= 0:
        return None

    # Garbage filtering
    if looks_like_garbage(item_name):
        return None
    if not capacity_strict_match(master, item_name):
        return None

    # postageFlag: 0=shipping included, 1=shipping NOT included (add DEFAULT_SHIPPING_YEN) 
    postage_flag = safe_int(item.get("postageFlag", 0), 0)
    shipping = DEFAULT_SHIPPING_YEN if postage_flag == 1 else 0

    # pointRate is percent (e.g. 2 -> 2%). Not all campaigns are reflected; Phase1 uses what API returns.
    point_rate_percent = safe_float(item.get("pointRate", 0.0), 0.0)
    point_rate = max(0.0, min(1.0, point_rate_percent / 100.0))
    point_rate = max(0.0, min(1.0, point_rate + EXTRA_POINT_RATE))

    denom = master.capacity_kg * master.protein_ratio
    if denom <= 0:
        return None

    protein_cost = ((raw_price + shipping) * (1.0 - point_rate)) / denom

    return OfferRow(
        date=date,
        canonical_id=master.canonical_id,
        item_code=item_code,
        shop_name=shop_name,
        raw_price=raw_price,
        shipping_cost=shipping,
        point_rate=point_rate,
        protein_cost=protein_cost,
        item_url=item_url,
        item_name=item_name,
        image_url=image_url,
    )


def classify_item_filter(master: MasterItem, item: Dict[str, Any], seen_keys: set) -> Tuple[Optional[OfferRow], Optional[str]]:
    item_code = str(item.get("itemCode", "")).strip()
    shop_name = str(item.get("shopName", "")).strip()
    raw_price = safe_int(item.get("itemPrice", 0), 0)
    item_name = str(item.get("itemName", "")).strip()

    if not item_code or not shop_name or raw_price <= 0:
        return None, "missing_required_or_invalid_price"
    if looks_like_garbage(item_name):
        return None, "excluded_keyword"
    if not capacity_strict_match(master, item_name):
        return None, "capacity_mismatch"

    offer = compute_offer(master, item)
    if not offer:
        return None, "invalid_offer"

    key = (offer.date, offer.canonical_id, offer.item_code, offer.shop_name)
    if key in seen_keys:
        return None, "duplicate"

    return offer, None


# =========================
# Main
# =========================
def main():
    print("ACCESS_KEY len:", len(os.environ.get("RAKUTEN_ACCESS_KEY","")))
    print("APP_ID:", os.environ.get("RAKUTEN_APP_ID", "")[:6], "len=", len(os.environ.get("RAKUTEN_APP_ID","")))
    print("ENDPOINT:", RAKUTEN_ENDPOINT)
    today = jst_today_str()
    yesterday = (jst_date() - timedelta(days=1)).isoformat()

    master_ws, hist_ws, min_ws = open_sheets()
    masters = read_master(master_ws)
    if not masters:
        raise RuntimeError("Master_List is empty or missing required columns.")

    masters = [m for m in masters if is_explosion_3kg_target(m)]
    if not masters:
        raise RuntimeError("Target product (ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³3kg) not found in Master_List.")

    # Read minima from Min_Summary only (fast)
    yday_min = read_min_summary(min_ws, yesterday)   # {cid: (cost, shop, url)}
    alltime_min = read_alltime_min(min_ws)          # {cid: (cost, shop, url)}

    all_offers: List[OfferRow] = []
    notify_payloads: List[Tuple[str, List[str]]] = []
    best_offers_for_ranking: List[OfferRow] = []
    marketing_reports: List[Tuple[MasterItem, OfferRow, PriceChangeReport]] = []
    run_errors: List[str] = []

    for m in masters:
        time.sleep(REQUEST_SLEEP_SEC)

        # Fetch many, then compute effective cost and keep best STORE_HITS
        items, api_total_count = rakuten_search_multi_pages(m.search_keyword, total_hits=FETCH_HITS)
        print(
            "DEBUG fetch:",
            f"canonical_id={m.canonical_id}",
            f"keyword={m.search_keyword}",
            f"api_total_count={api_total_count}",
            f"fetched_items={len(items)}",
            f"sample={(items[0].get('itemName', '')[:60] if items else 'NONE')}",
        )

        seen = set()  # (date,cid,item_code,shop_name)
        offers_for_this: List[OfferRow] = []
        filter_drop_counts: Dict[str, int] = {
            "missing_required_or_invalid_price": 0,
            "excluded_keyword": 0,
            "capacity_mismatch": 0,
            "invalid_offer": 0,
            "duplicate": 0,
        }

        for it in items:
            offer, dropped_reason = classify_item_filter(m, it, seen)
            if not offer:
                if dropped_reason:
                    filter_drop_counts[dropped_reason] += 1
                continue
            key = (offer.date, offer.canonical_id, offer.item_code, offer.shop_name)
            seen.add(key)
            offers_for_this.append(offer)

        accepted_before_store_limit = len(offers_for_this)
        dropped_by_store_limit = max(0, accepted_before_store_limit - STORE_HITS)
        filter_drop_counts["store_hits_limit"] = dropped_by_store_limit

        print(
            "DEBUG filter:",
            f"canonical_id={m.canonical_id}",
            f"input_items={len(items)}",
            f"accepted_before_store_limit={accepted_before_store_limit}",
            "drop_counts=" + json.dumps(filter_drop_counts, ensure_ascii=False),
        )

        # Sort by effective cost (protein_cost) and keep top STORE_HITS
        offers_for_this.sort(key=lambda x: x.protein_cost)
        offers_for_this = offers_for_this[:STORE_HITS]

        # Append to history buffer
        all_offers.extend(offers_for_this)

        # Determine today's best and upsert Min_Summary
        if offers_for_this:
            best = offers_for_this[0]
            ranking_offers = offers_for_this[:RANKING_N]
            best_offers_for_ranking.append(best)
            if best.image_url:
                print(f"INFO selected best_offer.image_url canonical_id={m.canonical_id} url={best.image_url}")
            else:
                print(f"WARNING best_offer.image_url is empty canonical_id={m.canonical_id}")
            print(
                f"INFO ranking_count={len(ranking_offers)} hero_count={min(HERO_K, len(ranking_offers))} canonical_id={m.canonical_id}"
            )
            upsert_today_min(min_ws, today, m.canonical_id, best.protein_cost, best.shop_name, best.item_url)
            marketing_reports.append(
                (m, best, build_marketing_report(m, best, hist_ws, today, yesterday, ranking_offers=ranking_offers))
            )

            y_best = yday_min.get(m.canonical_id)
            a_best = alltime_min.get(m.canonical_id)

            changed_shop = (y_best is not None) and (best.shop_name != y_best[1])
            new_alltime_low = (a_best is None) or (best.protein_cost < a_best[0])

            if changed_shop or new_alltime_low:
                top3 = offers_for_this[:3]
                lines = [
                    f"- canonical_id: `{m.canonical_id}` / keyword: {m.search_keyword}",
                    f"- ä»Šæ—¥ã®æœ€å®‰: **{best.shop_name}** / å®Ÿè³ª(ã‚¿ãƒ³ãƒ‘ã‚¯1kgã‚ãŸã‚Š): **{best.protein_cost:,.0f}å††**",
                    f"- ä¾¡æ ¼: {best.raw_price:,}å†† é€æ–™åŠ ç®—:{best.shipping_cost:,}å†† pt:{best.point_rate*100:.1f}%",
                    f"- å•†å“: {best.item_name[:100]}",
                    f"- URL: {best.item_url}",
                ]
                if y_best:
                    lines.append(f"- æ˜¨æ—¥ã®æœ€å®‰: {y_best[1]} / {y_best[0]:,.0f}å††")
                if a_best:
                    lines.append(f"- éå»æœ€å®‰: {a_best[1]} / {a_best[0]:,.0f}å††")

                lines.append("")
                lines.append("Top3:")
                for i, o in enumerate(top3, 1):
                    lines.append(
                        f"{i}. {o.shop_name} / {o.protein_cost:,.0f}å†† (ä¾¡æ ¼{o.raw_price:,}+é€æ–™{o.shipping_cost:,}, pt{o.point_rate*100:.1f}%)"
                    )

                title = "ã€éå»æœ€å®‰æ›´æ–°ã€‘" if new_alltime_low else "ã€æœ€å®‰ã‚·ãƒ§ãƒƒãƒ—å…¥ã‚Œæ›¿ã‚ã‚Šã€‘"
                notify_payloads.append((f"{title} {m.canonical_id} ({today})", lines))

    # Write to Price_History
    print(f"DEBUG append: rows_to_append={len(all_offers)}")
    if len(all_offers) == 0:
        msg = "No offers to append after filtering."
        if STRICT_MODE:
            raise RuntimeError(f"STRICT_MODE=true: {msg}")
        print(f"WARNING: {msg} STRICT_MODE=false so run is treated as success.")

    append_history(hist_ws, all_offers)

    # Send notifications
    for title, lines in notify_payloads:
        discord_notify(title, lines)

    # Generate and notify revenue-maximized posting drafts for Explosion 3kg
    hatena_result = HatenaPostResult(ok=True, status_code=None, endpoint="", message="skipped")
    for m, best, report in marketing_reports:
        diff_line = (
            f"{report.diff_yen:+,}å†† ({report.diff_pct:+.1f}%)"
            if report.diff_yen is not None and report.diff_pct is not None
            else "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
        )
        lines = [
            f"- product: {m.canonical_id} / ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³3kg",
            f"- today: {report.today_price:,}å††",
            f"- å‰æ—¥æ¯”: {diff_line}",
            f"- 30æ—¥æœ€å®‰: {'æ›´æ–°' if report.is_30d_low else 'æœªæ›´æ–°'}"
            + (f" ({report.min_30d_price:,}å††)" if report.min_30d_price is not None else ""),
            f"- level: {report.level}",
            f"- variant: {report.variant} ({report.date_jst} {report.weekday_jst})",
            f"- image: {'æ¡ç”¨' if report.image_selected else 'æœªå–å¾—'}",
            "",
            "[äººåˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆè¦ç´„ï¼‰]",
        ] + report.persona_summary_lines + [
            "",
            "[XæŠ•ç¨¿æ¡ˆ]",
            report.x_text,
            "",
            "[HatenaæŠ•ç¨¿Markdownæ¡ˆ]",
            report.hatena_markdown[:1200],
        ]
        discord_notify("ğŸ“ æŠ•ç¨¿æ¡ˆé€šçŸ¥ï¼ˆRakuten Protein Trackerï¼‰", lines)

        print(
            "INFO marketing:",
            f"variant={report.variant}",
            f"date_jst={report.date_jst}",
            f"weekday_jst={report.weekday_jst}",
            f"image_url_status={'æ¡ç”¨' if report.image_selected else 'æœªå–å¾—'}",
        )

        hatena_result = post_top3_to_hatena(report.hatena_markdown)
        if not hatena_result.ok:
            run_errors.append(
                f"Hatena draft post failed (status={hatena_result.status_code}, endpoint={hatena_result.endpoint}): {hatena_result.message}"
            )

    summary_lines = [
        f"- date: {today}",
        f"- appended rows: {len(all_offers)}",
        f"- change notifications: {len(notify_payloads)}",
        f"- marketing drafts: {len(marketing_reports)}",
        f"- hatena status: {'OK' if hatena_result.ok else 'NG'}",
        f"- hatena endpoint: {hatena_result.endpoint or '(not built)'}",
        f"- hatena http_status: {hatena_result.status_code if hatena_result.status_code is not None else 'N/A'}",
    ]
    if run_errors:
        summary_lines.append("- errors:")
        for err in run_errors:
            summary_lines.append(f"  - {err[:300]}")

    discord_notify("ğŸ“Š Rakuten protein tracker summary", summary_lines)

    if run_errors:
        raise RuntimeError("; ".join(run_errors))

    print(f"OK: appended {len(all_offers)} rows, notified {len(notify_payloads)} items.")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        msg = "".join(traceback.format_exception(type(e), e, e.__traceback__))[-1800:]
        discord_notify("âŒ Rakuten protein tracker failed", [f"```{msg}```"])
        raise
